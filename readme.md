# üìö [ESTUDO] Gera√ß√£o de Texto com Modelos de Linguagem Quantizados

Este √© um projetinho de estudo com foco em aprendizado pr√°tico sobre **Modelos de Linguagem (LLMs)**, **uso da GPU**, **quantiza√ß√£o 4-bit**, e boas pr√°ticas na constru√ß√£o de pipelines com a biblioteca `transformers` da Hugging Face.

---

## Objetivo

Explorar, estudar e testar o uso de modelos de linguagem otimizados para uso local com **quantiza√ß√£o em 4 bits**, reduzindo o uso de mem√≥ria e viabilizando a execu√ß√£o em m√°quinas com GPUs mais modestas.

---

## Tecnologias Usadas

- **Transformers** da Hugging Face
- **Modelo:** [`microsoft/Phi-3-mini-4k-instruct`](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) ‚Äì um modelo poderoso, leve e recente
- **Quantiza√ß√£o:** `BitsAndBytesConfig` com `load_in_4bit=True`
- **CUDA** para uso de GPU
- `.env` para configura√ß√£o segura do Hugging Face token

---
